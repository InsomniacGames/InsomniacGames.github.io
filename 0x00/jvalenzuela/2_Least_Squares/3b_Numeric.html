<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    <style>

    tbody tr:nth-child(odd) {
        background-color: #ccc;
    }
    table {
        border: 1px solid black;
        border-collapse: collapse;
    }
    tr td {
        border-bottom:1pt solid black;
        padding:6px;
    }
    </style>
</head>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: { extensions: ["AMSsymbols.js"] },
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<h2 id="numericissues">Numeric issues</h2>

<h3 id="solvingthenormalequations">Solving the normal equations</h3>

<p>Calculating <span class="math">\((A^TA)^{-1}A^T\)</span> is kind of overkill. In general there are better alternatives than <a href="http://www.johndcook.com/blog/2010/01/19/dont-invert-that-matrix/">calculating the inverse</a> of a matrix to solve a system of equations. Gaussian elimation is simple alternative. Although it operates in <span class="math">\(O(n^3)\)</span> steps it necessarily requires fewer operations than calculating the inverse and is numerically more stable.</p>

<p>Provided the columns of <span class="math">\(A\)</span> are independent, then <span class="math">\(A^TA\)</span> will have an inverse. However, even if the columns of <span class="math">\(A\)</span> are independent, it&#8217;s possible for <span class="math">\(A\)</span> to be <a href="https://en.wikipedia.org/wiki/Condition_number">ill-conditioned</a>, in which case orthogonal decomposition techniques are a slower but more stable route. <a href="http://www.math.uconn.edu/~leykekhman/courses/MATH3795/Lectures/Lecture_8_Linear_least_squares_orthogonal_matrices.pdf">Dmitriy Leykekhman</a> describes using <span class="math">\(QR\)</span> decomposition to solve linear least squares. In the general case, <span class="math">\(A\)</span> is an <span class="math">\(m\times{}n\)</span> matrix with rank <span class="math">\(n\)</span>. Given <span class="math">\(A = QR\)</span>, in order to solve <span class="math">\(A\textbf{x} = \textbf{b}\)</span>, we determine <span class="math">\(\textbf{x} = P\textbf{y}\)</span> such that:</p>

<p><span class="math">\(R\textbf{y} = \textbf{c}\)</span></p>

<p><span class="math">\(Q^T\textbf{b} = \begin{pmatrix}\textbf{c} \\ \textbf{d} \end{pmatrix}\)</span></p>

<p>and <span class="math">\(P\)</span> is the permutation matrix such that</p>

<p><span class="math">\(Q^T A P = \begin{pmatrix}R \\ 0 \end{pmatrix}\)</span></p>

<h3 id="leastsquarespolynomial">Least Squares Polynomial</h3>

<p>The polynomial fit problems I&#8217;ve described so far are discrete: they match at a particular set of points. There is more sophisticated use of least squares which operates continuously on a function over some domain. This technique generates the Least Squares Polynomial.</p>

<p><a href="http://www.math.niu.edu/~dattab/MATH435.2013/APPROXIMATION.pdf">Biswa Nath Datta</a> offers a detailed derivation for the Least Squares Polynomial problem. The goal in to minimize the residual continuously over the entire interval, in essence finding the closest overall polynomial approximation. The objective function minimized in this case is <span class="math">\(E = \int_a^b[f(x)-P_n]^2dx\)</span>. By calculating the derivative of <span class="math">\(E\)</span> and setting to 0, the normal equations work out to be (using the powers of x as our basis):</p>

<p><span class="math">\(a_0\int_a^b1dx + a_1\int_a^bxdx + \cdots + a_n\int_a^bx^ndx = \int_a^bf(x)dx\)</span></p>

<p><span class="math">\(\cdots\)</span></p>

<p><span class="math">\(a_0\int_a^bx^idx + a_1\int_a^bx^{i+1}dx + \cdots + a_n\int_a^bx^{i+n}dx = \int_a^bx^if(x)dx\)</span></p>

<p>for <span class="math">\(i=1,2,3,\cdots,n\)</span></p>

<p>Substituting <span class="math">\(s_i\)</span> for the integrals on the left and <span class="math">\(b_i\)</span> for those on the right gives us</p>

<p><span class="math">\(\begin{pmatrix}s_0 & s_1 & \cdots & s_n \\ s_1 & s_2 & \cdots & s_{n+1} \\ \vdots & \vdots & \ddots & \vdots \\ s_n & s_{n+1} & \cdots & s_{2n}\end{pmatrix} \begin{pmatrix}a_0 \\ a_1 \\ \vdots \\ a_n\end{pmatrix} = \begin{pmatrix}b_0 \\ b_1 \\ \vdots \\ b_n\end{pmatrix}\)</span></p>

<p>&#8230;or <span class="math">\(S\textbf{a} = \textbf{b}\)</span>.</p>

<p>It is generally recommended to, instead of using the powers of x to generate <span class="math">\(S\)</span>, to instead use an orthogonal polynomial basis. This is because <span class="math">\(S\)</span> above is often ill-conditioned when using the powers of x, leading to instability and the accumulation of numeric errors. As an alternative, an orthogonal polynomial basis, such as the <a href="https://en.wikipedia.org/wiki/Legendre_polynomials">Legendre polynomials</a>, can produce a stable <span class="math">\(S\)</span> and are reasonably convienient to work with not least because they can be derived iteratively.</p>

<p><img align="center" src="3_OrthogonalBasis.png"></p>

<p>For example, consider the case of fitting a quadratic polynomial to <span class="math">\(e^x\)</span>. Biswa Nath Datta computes a solution of using an orthogonal basis, <span class="math">\(p(x) = \frac{1}{2}(e-\frac{1}{e}) + \frac{3}{e}x + (\frac{15 (e^2-7)}{4 e})(x^2-\frac{1}{3})\)</span> and the powers-of-x basis, <span class="math">\(p(x) = 0.9963 + 1.1037x + 0.5368x^2\)</span> respectively. Measuring the error with <span class="math">\(\int_{-1}^{1}(e^x-p(x))^2\)</span> gives the relative values of approximately <span class="math">\(0.001441\)</span> and <span class="math">\(0.00193\)</span> respectively. In general we expect the polynomials to be very close. In some circumstances minimizing the overall error is less important than minimizing the error at a particular set of points, in which case the discrete method may be preferable.</p>

<h3 id="choiceofx">Choice of x</h3>

<p>While Least Squares is designed to be robust against errors in y, it is not particularly good about errors in x (in statistics terms we say that the regressors - the elements of <span class="math">\(\textbf{x}\)</span> - must be <em>exogenous</em>). The errors it fares best against are random and normally distributed - any consistent bias in measurement cannot be corrected.</p>

<p>When generating a least squares polynomial with an orthogonal polynomial basis, care must be taken though to use the interval over which the basis is orthogonal. Legendre polynomials, for example, are orthogonal over the domain <span class="math">\(-1 \leq x \leq 1\)</span>. Using least squares in this case requires either restricting the domain or using a change of variables to effect the same.</p>

<h3 id="measuringerror">Measuring error</h3>

<p>When doing function approximation (or modeling), we usually need to measure how good our approximation is. This is especially useful when we&#8217;re generating a polynomial approximation but aren&#8217;t sure what the right degree is: incrementally increasing the degree and stopping when the error inflects can be our best option. There are several standard methods of measuring error, and the <a href="https://en.wikipedia.org/wiki/Root_mean_square">Root Mean Square</a> - RMS - is typical.</p>

<p><span class="math">\(E_2 = \sqrt{{\sum_{i=1}}^{n} (y_i - f(x_i))^2}\)</span></p>

<p>In the case where we&#8217;re trying to build a simple model of an affine function <span class="math">\(y = mx + b\)</span>, the <em>coerrelation coefficient</em> tells us how closely the data ends up fitting the straight line. The correlation coefficient is labeled <span class="math">\(r\)</span>.</p>

<p><span class="math">\[r = \sqrt{\frac{\sum_{i} [(b+mx_i)-\tilde{y}]^2}{\sum_{i}(y_i - \tilde{y})^2}}\]</span></p>

<p>&#8230;where <span class="math">\(\tilde{y}\)</span> is the mean value of the dependent variable y, <span class="math">\(r\)</span> ranges from +1 to &#8211;1, and the extreme values of <span class="math">\(r\)</span> indicate a perfectly linear relationship (the sign indicating the slope).</p>

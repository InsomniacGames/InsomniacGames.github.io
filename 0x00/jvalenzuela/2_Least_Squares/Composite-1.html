<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <style>
      tbody {
        font-size: 80%;
      }
  
      tbody tr:nth-child(odd) {
        background-color: #ccc;
      }
      table {
        border: 1px solid black;
        border-collapse: collapse;
      }
      tr td {
        border-bottom:1pt solid black;
        padding:6px;
      }
      th {
        padding:6px;
      }
  </style>
  <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  <link rel = "stylesheet" type = "text/css" href = "../../../style.css" />
</head>
<body>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: { extensions: ["AMSsymbols.js"] },
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: { extensions: ["AMSsymbols.js"] },
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<h1 id="leastsquarescontinued">Least Squares (continued)</h1>
<p>This is the second part of a two-part writeup about that venerable optimization technique, least squares. I sought mostly to cover stability issues and some interesting variants like Least Squares Polynomial generation and equality constrained Least Squares. I’d suggest reading through the included references for more detail, especially for the continuous least squares polynomial technique where a detailed derivation is kind of lengthy. Also presented is a solution for ellipse fitting using the quadratic form which I think is simpler than the parametric equation approach.</p>
<p>I’d appreciate any corrections or other feedback.</p>
<h2 id="numericissues">Numeric issues</h2>
<h3 id="solvingthenormalequations">Solving the normal equations</h3>
<p>Calculating <span class="math">\((A^TA)^{-1}A^T\)</span> is kind of overkill. In general there are better alternatives than <a href="http://www.johndcook.com/blog/2010/01/19/dont-invert-that-matrix/">calculating the inverse</a> of a matrix to solve a system of equations. Gaussian elimation is simple alternative. Although it operates in <span class="math">\(O(n^3)\)</span> steps it necessarily requires fewer operations than calculating the inverse and is numerically more stable.</p>
<p>Provided the columns of <span class="math">\(A\)</span> are independent, then <span class="math">\(A^TA\)</span> will have an inverse. However, even if the columns of <span class="math">\(A\)</span> are independent, it’s possible for <span class="math">\(A\)</span> to be <a href="https://en.wikipedia.org/wiki/Condition_number">ill-conditioned</a>, in which case orthogonal decomposition techniques are a slower but more stable route. <a href="http://www.math.uconn.edu/~leykekhman/courses/MATH3795/Lectures/Lecture_8_Linear_least_squares_orthogonal_matrices.pdf">Dmitriy Leykekhman</a> describes using <span class="math">\(QR\)</span> decomposition to solve linear least squares. In the general case, <span class="math">\(A\)</span> is an <span class="math">\(m\times{}n\)</span> matrix with rank <span class="math">\(n\)</span>. Given <span class="math">\(A = QR\)</span>, in order to solve <span class="math">\(A\textbf{x} = \textbf{b}\)</span>, we determine <span class="math">\(\textbf{x} = P\textbf{y}\)</span> such that:</p>
<p><span class="math">\(R\textbf{y} = \textbf{c}\)</span></p>
<p><span class="math">\(Q^T\textbf{b} = \begin{pmatrix}\textbf{c} \\ \textbf{d} \end{pmatrix}\)</span></p>
<p>and <span class="math">\(P\)</span> is the permutation matrix such that</p>
<p><span class="math">\(Q^T A P = \begin{pmatrix}R \\ 0 \end{pmatrix}\)</span></p>
<h3 id="leastsquarespolynomial">Least Squares Polynomial</h3>
<p>The polynomial fit problems I’ve described so far are discrete: they match at a particular set of points. There is more sophisticated use of least squares which operates continuously on a function over some domain. This technique generates the Least Squares Polynomial.</p>
<p><a href="http://www.math.niu.edu/~dattab/MATH435.2013/APPROXIMATION.pdf">Biswa Nath Datta</a> offers a detailed derivation for the Least Squares Polynomial problem. The goal in to minimize the residual continuously over the entire interval, in essence finding the closest overall polynomial approximation. The objective function minimized in this case is <span class="math">\(E = \int_a^b[f(x)-P_n]^2dx\)</span>. By calculating the derivative of <span class="math">\(E\)</span> and setting to 0, the normal equations work out to be (using the powers of x as our basis):</p>
<p><span class="math">\(a_0\int_a^b1dx + a_1\int_a^bxdx + \cdots + a_n\int_a^bx^ndx = \int_a^bf(x)dx\)</span></p>
<p><span class="math">\(\cdots\)</span></p>
<p><span class="math">\(a_0\int_a^bx^idx + a_1\int_a^bx^{i+1}dx + \cdots + a_n\int_a^bx^{i+n}dx = \int_a^bx^if(x)dx\)</span></p>
<p>for <span class="math">\(i=1,2,3,\cdots,n\)</span></p>
<p>Substituting <span class="math">\(s_i\)</span> for the integrals on the left and <span class="math">\(b_i\)</span> for those on the right gives us</p>
<p><span class="math">\(\begin{pmatrix}s_0 &amp; s_1 &amp; \cdots &amp; s_n \\ s_1 &amp; s_2 &amp; \cdots &amp; s_{n+1} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ s_n &amp; s_{n+1} &amp; \cdots &amp; s_{2n}\end{pmatrix} \begin{pmatrix}a_0 \\ a_1 \\ \vdots \\ a_n\end{pmatrix} = \begin{pmatrix}b_0 \\ b_1 \\ \vdots \\ b_n\end{pmatrix}\)</span></p>
<p>…or <span class="math">\(S\textbf{a} = \textbf{b}\)</span>.</p>
<p>It is generally recommended to, instead of using the powers of x to generate <span class="math">\(S\)</span>, to instead use an orthogonal polynomial basis. This is because <span class="math">\(S\)</span> above is often ill-conditioned when using the powers of x, leading to instability and the accumulation of numeric errors. As an alternative, an orthogonal polynomial basis, such as the <a href="https://en.wikipedia.org/wiki/Legendre_polynomials">Legendre polynomials</a>, can produce a stable <span class="math">\(S\)</span> and are reasonably convienient to work with not least because they can be derived iteratively.</p>
<p><img src="3_OrthogonalBasis.png" /></p>
<p>For example, consider the case of fitting a quadratic polynomial to <span class="math">\(e^x\)</span>. Biswa Nath Datta computes a solution of using an orthogonal basis, <span class="math">\(p(x) = \frac{1}{2}(e-\frac{1}{e}) + \frac{3}{e}x + (\frac{15 (e^2-7)}{4 e})(x^2-\frac{1}{3})\)</span> and the powers-of-x basis, <span class="math">\(p(x) = 0.9963 + 1.1037x + 0.5368x^2\)</span> respectively. Measuring the error with <span class="math">\(\int_{-1}^{1}(e^x-p(x))^2\)</span> gives the relative values of approximately <span class="math">\(0.001441\)</span> and <span class="math">\(0.00193\)</span> respectively. In general we expect the polynomials to be very close. In some circumstances minimizing the overall error is less important than minimizing the error at a particular set of points, in which case the discrete method may be preferable.</p>
<h3 id="choiceofx">Choice of x</h3>
<p>While Least Squares is designed to be robust against errors in y, it is not particularly good about errors in x (in statistics terms we say that the regressors - the elements of <span class="math">\(\textbf{x}\)</span> - must be <em>exogenous</em>). The errors it fares best against are random and normally distributed - any consistent bias in measurement cannot be corrected.</p>
<p>When generating a least squares polynomial with an orthogonal polynomial basis, care must be taken though to use the interval over which the basis is orthogonal. Legendre polynomials, for example, are orthogonal over the domain <span class="math">\(-1 \leq x \leq 1\)</span>. Using least squares in this case requires either restricting the domain or using a change of variables to effect the same.</p>
<h3 id="measuringerror">Measuring error</h3>
<p>When doing function approximation (or modeling), we usually need to measure how good our approximation is. This is especially useful when we’re generating a polynomial approximation but aren’t sure what the right degree is: incrementally increasing the degree and stopping when the error inflects can be our best option. There are several standard methods of measuring error, and the <a href="https://en.wikipedia.org/wiki/Root_mean_square">Root Mean Square</a> - RMS - is typical.</p>
<p><span class="math">\(E_2 = \sqrt{{\sum_{i=1}}^{n} (y_i - f(x_i))^2}\)</span></p>
<p>In the case where we’re trying to build a simple model of an affine function <span class="math">\(y = mx + b\)</span>, the <em>coerrelation coefficient</em> tells us how closely the data ends up fitting the straight line. The correlation coefficient is labeled <span class="math">\(r\)</span>.</p>
<p><span class="math">\[r = \sqrt{\frac{\sum_{i} [(b+mx_i)-\tilde{y}]^2}{\sum_{i}(y_i - \tilde{y})^2}}\]</span></p>
<p>…where <span class="math">\(\tilde{y}\)</span> is the mean value of the dependent variable y, <span class="math">\(r\)</span> ranges from +1 to –1, and the extreme values of <span class="math">\(r\)</span> indicate a perfectly linear relationship (the sign indicating the slope).</p>
<h2 id="weightedlinearsquares">Weighted Linear Squares</h2>
<p>In our examples heretofore we’ve been assuming that each measurement - each row in <span class="math">\(\textbf{y}\)</span> - has equal accuracy. But sometimes we know that some measurements (and hence data points) are more reliable than others. To bias the final <span class="math">\(\textbf{x}^*\)</span> to work harder to minimize the error for those measurements, we can add a weight to equations that calculate the residual.</p>
<p>Consider the sum of squares error <span class="math">\(E^2 = \sum_i^n (y_i-a_i x_i)^2\)</span>. Weighting the individual datapoints consists of multiplying each equation with an additional paramters, <span class="math">\(w_i\)</span>, such that <span class="math">\(w_i &gt; w_j\)</span> implies that we have more confidence in data point <span class="math">\(i\)</span> than <span class="math">\(j\)</span>. Ultimately this gives us <span class="math">\(E^2 = \sum_i^n w_i (y_i-a_i x_i)^2\)</span>, or in matrix form this is equivalent to <span class="math">\((\textbf{y}-Ax)^T W (\textbf{y}-Ax)\)</span> where</p>
<p><span class="math">\(W = \begin{pmatrix} w_1 &amp; 0 &amp; \cdots \\ 0 &amp; w_2 &amp; \cdots \\ &amp; \ddots &amp; \\ \cdots &amp; \cdots &amp; w_3 \\ \end{pmatrix}\)</span>.</p>
<p>Performing least squares with a change of variables, <span class="math">\(\tilde{A} = AW\)</span>, <span class="math">\(\tilde{\textbf{y}} = W\textbf{y}\)</span> gives us the <em>weighted normal equations</em>:</p>
<p><span class="math">\((\tilde{A}^T\tilde{A})\textbf{x} =\tilde{A}^T\tilde{\textbf{b}}\)</span>.</p>
<p><span class="math">\((A^TW^TWA)\textbf{x} =A^TW^TW\textbf{b}\)</span>.</p>
<p>Alternatively, equating the derivative of the residual to 0, <span class="math">\(\frac{dE^2}{d\textbf{x}}=0\)</span>, will produce the same equations.</p>
<h2 id="equalityconstrainedlinearsquares">Equality constrained Linear Squares</h2>
<p>All the examples we’ve been looking at so far have been unconstrained: there are no restrictions on the values that <span class="math">\(\textbf{x}\)</span> can take. We can add equality constraints to our <span class="math">\(\min |A\textbf{x}=\textbf{y}|^2\)</span> easily, that is, with an analytic solution. (From <a href="http://livebooklabs.com/keeppies/c5a5868ce26b8125/6267fbb41c3f1521">Optimization models and applications</a>):</p>
<p>We express our equality constraints as the equation <span class="math">\(C\textbf{x} = \textbf{d}\)</span>. Assume that there is at least one feasible solution <span class="math">\(\textbf{x}_0\)</span> for <span class="math">\(C\textbf{x}=d\)</span>. Any attempt to move <span class="math">\(\textbf{x}\)</span> for the purpose of minimizing <span class="math">\(|A\textbf{x}=\textbf{y}|^2\)</span> by some vector <span class="math">\(\textbf{s}\)</span> must result in <span class="math">\(C(\textbf{x}_0 + \textbf{s}) = \textbf{d} \implies C\textbf{x}_0 + C\textbf{s} = \textbf{d}\)</span> to be correct. Since we must match <span class="math">\(C\textbf{x}_0 = \textbf{d}\)</span> exactly, <span class="math">\(C\textbf{s}\)</span> must be 0. Any attempt to change <span class="math">\(\textbf{x}\)</span> must occur along the nullspace of <span class="math">\(C\)</span>.</p>
<p>Let <span class="math">\(N\)</span> be the nullspace of <span class="math">\(C\)</span>. We can rephrase the above by saying we’re looking to minimize <span class="math">\(|A\textbf{x}=\textbf{y}|^2\)</span> where <span class="math">\(\textbf{x} = \textbf{x}_0 + N\textbf{z}\)</span>, <span class="math">\(\textbf{z}\)</span> is an arbitrary vector. Therefore the problem changes to minimize <span class="math">\(|\tilde{A}\textbf{z}=\tilde{\textbf{y}}|^2\)</span> where <span class="math">\(\tilde{A}=AN\)</span> and <span class="math">\(\tilde{y}=y-A\textbf{x}_0\)</span>. At this point we can plug our equation into our familiar normal equations and derive <span class="math">\(\textbf{x}\)</span>.</p>
<h3 id="anexample">An example</h3>
<p><img src="4_EllipsePoints.png" /></p>
<p>Here’s an example adapted from <a href="http://holyshit.tech/statistical-learning/2016/11/06/least-squares.html">Flavio Truzzi’s article on Least Squares</a>. Take these set of data points which were generated by adding random noise about the ellipse <span class="math">\(\frac{1}{2}x^2 + \frac{1}{3}x^2 = 1\)</span> rotated by 20 degrees.</p>
<p>As a reminder there are multiple representations of the same ellipse. Truzzi uses the parametric form of a rotated ellipse. About the origin, the parameteric form is:</p>
<p><span class="math">\(x(t) = a\ cos(t) cos(\theta) - b\ sin(t) sin(\theta)\)</span></p>
<p><span class="math">\(y(t) = a\ cos(t) sin(\theta) + b\ sin(t) cos(\theta)\)</span></p>
<p>in this specific case,</p>
<p><span class="math">\(x(t) = 2\ cos(t) cos(20^°) - 3\ sin(t) sin(20^°)\)</span></p>
<p><span class="math">\(y(t) = 2\ cos(t) sin(20^°) + 3\ sin(t) cos(20^°)\)</span></p>
<p>In standard form the coefficients are <span class="math">\(\frac{1}{a}\)</span>, <span class="math">\(\frac{1}{b}\)</span> respectively.</p>
<p>Solving the general case provides values for <span class="math">\(a\ cos(\theta)\)</span> and <span class="math">\(b\ sin(\theta)\)</span>, which is less than ideal. More problematic, it requires solving two least squares problems, one for x and y separately. In order to limit the solution with equality constraints we need to solve them together. Thankfully we can avoid these problems using the quadratic form of the same ellipse.</p>
<p><span class="math">\(a'x^2 + b'xy + c'y^2 = 1\)</span></p>
<p><span class="math">\(a'\)</span>, <span class="math">\(b'\)</span>, and <span class="math">\(c'\)</span> are related to the <span class="math">\(a\)</span>, <span class="math">\(b\)</span> and the angle of rotation in the parametric representation by a set of formulas described in Charles F. Van Loan in <a href="http://www.cs.cornell.edu/cv/OtherPdf/Ellipse.pdf">Using the ellipse to fit and enclose data points</a>.</p>
<p>Given a set of <span class="math">\(x\)</span>, <span class="math">\(y\)</span> data points, we can reconstruct <span class="math">\(a'\)</span>, <span class="math">\(b'\)</span>, and <span class="math">\(c'\)</span> by the following <span class="math">\(A\)</span> and <span class="math">\(\textbf{b}\)</span> least squares setup:</p>
<p><img src="4_EllipsePoints2.png" /></p>
<p><span class="math">\(\begin{pmatrix} x_1^2 &amp; x_1y_1 &amp; y_1^2 \\ x_2^2 &amp; x_2y_2 &amp; y_2^2 \\ \cdots \\ x_n^2 &amp; x_ny_n &amp; y_n^2 \\ \end{pmatrix} \begin{pmatrix}a' \\ b' \\ c'\end{pmatrix} = \begin{pmatrix}1 \\ 1 \\ \cdots \\ 1\end{pmatrix}\)</span></p>
<p>Converting back to parametric form gives us our original values for <span class="math">\(a\)</span> and <span class="math">\(b\)</span>. We also get the angle of rotation factored individually. Pretty nice!</p>
<p>Now onto equality constraints. Let’s say we wanted to restrict out solutions to those where <span class="math">\(a = b\)</span>. In other words we’re looking for the circle that best fits this group of points.</p>
<p>In matrix form, this constraint is expressed as <span class="math">\(\begin{pmatrix}1 &amp; 0 &amp; -1 \\ 0 &amp; 1 &amp; 0\end{pmatrix} \textbf{x} = \textbf{0}\)</span>. The middle unknown multiplies <span class="math">\(xy\)</span> and is responsible for the rotation, which we’ve set to unconditionally equal 0 with the second row. The first row states we want the coefficients for <span class="math">\(x^2\)</span> and <span class="math">\(y^2\)</span> to be equal. The nullspace of our constraint matrix, <span class="math">\(N\)</span>, is <span class="math">\(\begin{pmatrix}1 \\ 0 \\ 1\end{pmatrix}\)</span>.</p>
<p><img src="4_EllipsePoints3.png" /></p>
<p>By inspection, a feasible solution for <span class="math">\(x_0\)</span> is <span class="math">\(\begin{pmatrix}1 \\ 0 \\ 1\end{pmatrix}\)</span>. Thus the optimal solution can be found via least squares with <span class="math">\(A N = \begin{pmatrix} x_1^2 + y_1^2 \\ \cdots \\ x_n^2 + y_n^2 \\ \end{pmatrix}\)</span>, <span class="math">\(\tilde{\textbf{b}} = \textbf{b} - A\textbf{x}_0 = \begin{pmatrix} b_1 - x_1^2 + y_1^2 \\ \cdots \\ b_n - x_n^2 + y_n^2\end{pmatrix}\)</span>.</p>
<p>Since the arrived at solution is not for <span class="math">\(\textbf{x}\)</span> directly but <span class="math">\(\textbf{z}\)</span>, our final solution <span class="math">\(\textbf{x}\)</span> will be <span class="math">\(\textbf{x}_0 + N\textbf{z}\)</span>.</p>
<h2 id="thatsit">That’s it</h2>
<p>Although that’s all I wanted to cover for least squares, there’s a ton more to cover and most of it of practical use. Least Squares is one of the more basic optimization techniques but also one of the most robust and fastest. I’d really recommend going through the referenced materials to get an idea both about further applications but also things like the calculus based derivations of the normal equations which I alluded to but didn’t present.</p>
<h2 id="furtherreading">Further reading</h2>
<h3 id="flaviotruzzileastsquares.http:holyshit.techstatistical-learning20161106least-squares.html2016">Flavio Truzzi, Least Squares. http://holyshit.tech/statistical-learning/2016/11/06/least-squares.html, 2016</h3>
<p>Least squares introduction, including code, with elliptical fit of data using parametric representation.</p>
<h3 id="charlesf.vanloan.usingtheellipsetofitandenclosedatapoints.http:www.cs.cornell.educvotherpdfellipse.pdf2006">Charles F. Van Loan. Using the ellipse to fit and enclose data points. http://www.cs.cornell.edu/cv/OtherPdf/Ellipse.pdf, 2006</h3>
<p>Comprehensive coverage of ellipse representations including conversion.</p>
<h3 id="laurentelghaoui.ee127lecture7.https:people.eecs.berkeley.eduelghaouiteachingee127lectures7_ls_notes.pdf">Laurent El Ghaoui. EE127 Lecture 7. https://people.eecs.berkeley.edu/~elghaoui/Teaching/EE127/Lectures/7_ls_notes.pdf</h3>
<p>Succinct derivation of the technique for solving equality constrained least squares on slide 5.</p>
<h3 id="dmitriyleykekhmanmath3975lecture8.http:www.math.uconn.eduleykekhmancoursesmath3795lectureslecture_8_linear_least_squares_orthogonal_matrices.pdf">Dmitriy Leykekhman, MATH 3975 Lecture 8. http://www.math.uconn.edu/~leykekhman/courses/MATH3795/Lectures/Lecture_8_Linear_least_squares_orthogonal_matrices.pdf</h3>
<p>Describes QR decomposition for solving linear leasts squares in great detail.</p>
<h3 id="biswanathdattamath435chapter10.http:www.math.niu.edudattabmath435.2013approximation.pdf">Biswa Nath Datta, MATH 435 Chapter 10. http://www.math.niu.edu/~dattab/MATH435.2013/APPROXIMATION.pdf</h3>
<p>Excellent introduction to least squares approximation of functions.</p>
<h3 id="johnr.fanchimathrefresherforscientistsandengineers1998.">John R. Fanchi, Math Refresher for Scientists and Engineers, 1998.</h3>
<p>A compendious resource in general, all information about the Correlation Coefficient came from this.</p>
<h3 id="anthonyralstonphiliprabinowitzafirstcourseinnumericalanalysis1965.">Anthony Ralston, Philip Rabinowitz, A first course in numerical analysis, 1965.</h3>
<p>Good (if a bit dated) coverage of numerical and stability issues.</p>
<p>-- Joe Valenzuela (Senior Engine Programmer)</p>
</body>
</html>

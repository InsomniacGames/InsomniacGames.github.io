<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    <style>

    tbody tr:nth-child(odd) {
        background-color: #ccc;
    }
    table {
        border: 1px solid black;
        border-collapse: collapse;
    }
    tr td {
        border-bottom:1pt solid black;
        padding:6px;
    }
    </style>
</head>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: { extensions: ["AMSsymbols.js"] },
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<h2 id="theproblem">The Problem</h2>

<p><img align="left" src="random_points.png"></p>

<p>In statistics and function approximation, we often want to find a function (or group of functions) that models a system and matches a set of sampled data. Another goal may be to substitute a simpler function (typically a low order polynomial) for a complicated function (matching, if not everywhere, than hopefully at a set of particular data points).</p>

<p>The data, though, is usually a problem. Even when just trying to fit a simple line to a set of data points that should be linear, normal errors in measurement means there&#8217;s usually <em>no</em> line that exactly fits the points.</p>

<p>Consider a set of data points, <span class="math">\(\textbf{b}=\begin{pmatrix} b_1, ..., b_n \end{pmatrix}^T\)</span>, associated with an independent variable <span class="math">\(\textbf{x}=\begin{pmatrix} x_1, ..., x_n \end{pmatrix}^T\)</span>. Inspection of the data suggests that it was generated using a linear function of the form <span class="math">\(b = mx + c\)</span>. The linear function has two unknowns: <span class="math">\(m\)</span> and <span class="math">\(c\)</span>.</p>

<p>Consider <span class="math">\(n=2\)</span>, the case with two data points. With two samples and two unknowns, we&#8217;re presented with a familiar system of equations:</p>

<p><span class="math">\[m x_1 + c = b_1\]</span></p>

<p><span class="math">\[m x_2 + c = b_2\]</span></p>

<p>or, in matrix form,</p>

<p><span class="math">\[\begin{pmatrix} x_1 & 1 \\ x_2 & 1 \\ \end{pmatrix} \begin{pmatrix} m \\ c \end{pmatrix} = \begin{pmatrix} b_1 \\ b_2 \end{pmatrix}\]</span></p>

<p>For convenience we&#8217;ll label our matrix with <span class="math">\(A\)</span>. Solving for <span class="math">\(\textbf{x}\)</span> is a equivalent to computing <span class="math">\(A^{-1}\textbf{b}\)</span>. Assuming <span class="math">\(A\)</span> is invertible - equivalently, the matrix is full row rank and the rows are independent - this is possible.</p>

<p>Let&#8217;s say <span class="math">\(A\)</span> is a <span class="math">\(n{\times}2\)</span>, with <span class="math">\(n<2\)</span>. The number of rows is greater than the number of columns, and <span class="math">\(A\)</span> cannot be full row rank - every row cannot be independent. Said another way, given that two points define a line, a third point added will either not be on that line, thus leading to an inconsistent system, or it will be on the line, in which case it&#8217;s extraneous.</p>

<p>Thus if our measurements were perfectly accurate (and our function really is linear) then we can pick any two independent rows of <span class="math">\(A\)</span> and <span class="math">\(\textbf{b}\)</span> and recover <span class="math">\(m\)</span> and <span class="math">\(c\)</span>. More likely, our values in <span class="math">\(\textbf{b}\)</span> are not perfect, in which case different rows will solve for different results. There is probably no <span class="math">\(m\)</span> and <span class="math">\(c\)</span> which will fit every sample point.</p>

<p>If we can&#8217;t find an exact value for <span class="math">\(\begin{pmatrix} m \\ c \end{pmatrix}\)</span>, we can at least pick values for <span class="math">\(m\)</span> and <span class="math">\(c\)</span> that bring that line closest to the data points. To do that we&#8217;ll need a way of determining how close our current values are. Let <span class="math">\(\textbf{x} = \begin{pmatrix} m \\ c \end{pmatrix}\)</span>. Given a current estimate of <span class="math">\(x^*\)</span> for <span class="math">\(x\)</span>, how good an estimate is it?</p>

<p><span class="math">\[A\textbf{x}^* \approx \textbf{b}\]</span></p>

<p>To answer that question we need to be able to define and measure the error. Let <span class="math">\(\textbf{e}\)</span> be the error vector associated with each variable (technically, the residual, since we&#8217;re measuring error in the ordinate not the abscissa). We can define the difference between our generated value and the expected value in terms of <span class="math">\(\textbf{e}\)</span>:</p>

<p><span class="math">\[A\textbf{x}^* + \textbf{e} = \textbf{b} \implies \textbf{e} = \textbf{b} - A\textbf{x}^*\]</span></p>

<p>We can now use our definition for <span class="math">\(\textbf{e}\)</span> to help find <span class="math">\(\textbf{x}^*\)</span>. But before doing that, what does it actually mean?</p>

<h2 id="orthogonalcomponenttoprojectiononcolumnspaceofa">Orthogonal component to projection on column space of A</h2>

<p><img align="left" src="projection4.png"></p>

<p><span class="math">\(A\textbf{x}^{\star}\)</span>, necessarily, is a vector in the column space of <span class="math">\(A\)</span>. If <span class="math">\(\textbf{b}\)</span> is in the column space of <span class="math">\(A\)</span>, then <span class="math">\(A\textbf{x}^{\star}=b\)</span> and the problem is solved. Given <span class="math">\(\textbf{b}\)</span> not in the column space, though we have <span class="math">\(\textbf{e} \ne 0\)</span>. Indeed, our definition of <span class="math">\(\textbf{e}\)</span> suggets a method for its calculation: we factor <span class="math">\(\textbf{b}\)</span> into the portion in the column space of <span class="math">\(A\)</span> (<span class="math">\(A\textbf{x}^{\star}\)</span>), and the orthogonal component which connects <span class="math">\(A\textbf{x}^{\star}\)</span> to <span class="math">\(\textbf{b}\)</span>. That&#8217;s exactly what <span class="math">\(\textbf{e}\)</span> does.</p>

<p>Geometrically, then, connecting <span class="math">\(A\textbf{x}^{\star}\)</span> to <span class="math">\(\textbf{b}\)</span> requires a vector orthogonal to column space of <span class="math">\(A\)</span> - our <span class="math">\(\textbf{e}\)</span>. This geometry gives us the information we need to find a value for <span class="math">\(\textbf{e}\)</span>.</p>

<p>According to the <a href="http://mathworld.wolfram.com/FundamentalTheoremofLinearAlgebra.html">Fundamental Theorem of Linear Algebra</a>, the column space and left nullspace of a matrix are orthogonal. Therefore we know our <span class="math">\(\textbf{e}\)</span> to be in the left nullspace of <span class="math">\(A\)</span>, that is, <span class="math">\(A^T \textbf{e} = 0\)</span>.</p>

<p>So <span class="math">\(A^T\textbf{e} = 0 \implies A^T (\textbf{b} - A\textbf{x}^*) = 0\)</span>, or</p>

<p><span class="math">\[A^T\textbf{b} = A^T A\textbf{x}^*\]</span></p>

<p>This is the matrix form of the <em>normal equations</em>, which when solved give the least squares approximation. <span class="math">\(\textbf{x}^*\)</span> can be computed directly by first calculating <span class="math">\((A^T A)^{-1}\)</span> and then multiplying <span class="math">\(A^T\textbf{b}\)</span>. Provided the columns of <span class="math">\(A\)</span> are independent the inverse is guaranteed to exist. Although of course it&#8217;s possible and preferable to solve without calculating the inverse (as John D. Cook says, <a href="https://www.johndcook.com/blog/2010/01/19/dont-invert-that-matrix/">don&#8217;t invert that matrix</a>);</p>

<p>This doesn&#8217;t explain the name &#8220;Least Squares&#8221; though. Where does that come from?</p>

<p>The optimal solution <span class="math">\(x^*\)</span> minimizes the magnitude of <span class="math">\(\textbf{e}\)</span>, that is, <span class="math">\(|A\textbf{x}^* - \textbf{b}|\)</span>. Equivalently we can solve the square, which is simpler, <span class="math">\(|A\textbf{x}^* - \textbf{b}|^2\)</span>. Separating by terms leaves us minimizing <span class="math">\(\sum_{i=1}^{n} (a_i^T x_i - b_i)^2\)</span> where <span class="math">\(a_i^T\)</span> is the <span class="math">\(i^{th}\)</span> row vector in <span class="math">\(A\)</span>. It&#8217;s from this formulation - finding the least sum of the squares of the error - that the method gets its name.</p>

<p>If you&#8217;re familiar with matrix calculus you can take the partial derivative of <span class="math">\(|A\textbf{x} - \textbf{b}|^2\)</span>, or equivalently <span class="math">\((A\textbf{x} - \textbf{b})^T(A\textbf{x} - \textbf{b})\)</span>, to derive the Normal Equations.</p>

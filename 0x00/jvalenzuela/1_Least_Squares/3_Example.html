<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    <style>

    tbody tr:nth-child(odd) {
        background-color: #ccc;
    }
    table {
        border: 1px solid black;
        border-collapse: collapse;
    }
    tr td {
        border-bottom:1pt solid black;
        padding:6px;
    }
    </style>
</head>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: { extensions: ["AMSsymbols.js"] },
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<h2 id="examples">Examples</h2>

<h3 id="fittingdata">Fitting data</h3>

<p><img align="right" src="3_Table1Points.png"></p>

<table style="float:left;margin-right:50px;"><thead><tr><th>models</th><th>actors</th><th>ms</th></tr></thead>
<tbody>
<tr><td>5</td><td>8</td><td>28.085</td>
<tr><td>10</td><td>15</td><td>53.399</td>
<tr><td>15</td><td>23</td><td>81.681</td>
<tr><td>20</td><td>30</td><td>106.946</td>
<tr><td>25</td><td>38</td><td>135.185</td>
<tr><td>30</td><td>45</td><td>160.435</td>
<tr><td>35</td><td>53</td><td>188.703</td>
</tbody></table>

<p>Say we are given the following framerate data. We&#8217;re looking to relate the number of actors and models in a particular setup with the resulting framerate. Our intuition tells us that the millisecond cost will be proportional to the number of actors and models we have. A quick inspection of these points plotted suggest that they appear to lie in a line.</p>

<p>Given that observation, we&#8217;re looking for an equation of the form <span class="math">\(b = s x_0 + t x_1\)</span>, where <span class="math">\(x_0\)</span> is the number of models and <span class="math">\(x_1\)</span> the number of actors. To find the coefficients <span class="math">\(s\)</span> and <span class="math">\(t\)</span> using the normal equations, first we&#8217;ll need to form our system of equations in matrix form.</p>

<p>We&#8217;ve augmented our <span class="math">\(A\)</span> here with a column of ones, turning our modeling function into <span class="math">\(b = s x_0 + t x_1 + u\)</span>. This may not be necessary - it amounts to adding a y-intercept to our equation. If the y-intercept is 0 - if 0 actors and 0 models means a frame takes 0 ms to render - then <span class="math">\(u\)</span> will be 0 and we haven&#8217;t hurt anything. If there is some sort of fixed cost though, <span class="math">\(u\)</span> will capture it. It&#8217;s mostly harmless.</p>

<p>We express our equations in matrix form:</p>

<p><span class="math">\(\begin{pmatrix} 5&8&1\\10&15&1\\15&23&1\\20&30&1\\25&38&1\\30&45&1\\35&53&1\\\end{pmatrix} \textbf{x} = \begin{pmatrix}28.085\\53.399\\81.681\\106.946\\135.185\\160.435\\188.703\\\end{pmatrix}\)</span>
<span class="math">\(\implies A\textbf{x} = \textbf{b}\)</span></p>

<p>Armed with A, we rearrange terms in the normal equations to give us <span class="math">\((A^TA)^{-1}A^T\)</span>:</p>

<p><span class="math">\(\begin{pmatrix}
-0.7714285714280663 & 0.9857142857136317 & -0.7571428571423645 & 0.9999999999993371 & -0.7428571428566592 & 1.0142857142850423 & -0.7285714285709538\\\
0.4999999999996636 & -0.6666666666662298 & 0.49999999999966716 & -0.6666666666662227 & 0.49999999999968137 & -0.6666666666662227 & 0.49999999999968137\\\
0.42857142857152286 & 0.6190476190474968 & 0.1428571428572355 & 0.3333333333332086 & -0.1428571428570492 & 0.04761904761892388 & -0.42857142857133745\\\
\end{pmatrix}\)</span></p>

<p>Multiplying through to <span class="math">\(\textbf{b}\)</span> and taking three digits of precision we get <span class="math">\(\textbf{x}=\begin{pmatrix}0.89 \\ 2.97 \\ -0.13\end{pmatrix}\)</span>. So actors are twice as expensive as models according to our data.</p>

<h3 id="fittingcurves">Fitting curves</h3>

<p><img align="left" src="3_Table2aPoints.png"></p>

<p>Sometimes the best model for a set of data isn&#8217;t a linear function but a polynomial of higher order. Function approximation is a deep topic replete with interesting details but for the moment let&#8217;s see how well we can do with our humble least squares.</p>

<p>To the left is a set of data points that is definitely not linear. The general shape of the data is (half) parabolic, so we&#8217;re probably looking for a quadratic eqution as our model.</p>

<p>We construct <span class="math">\(A\)</span> a bit differently in this case. We&#8217;re looking for a set of coefficients <span class="math">\(a\)</span>, <span class="math">\(b\)</span>, <span class="math">\(c\)</span> to minimize the residual in <span class="math">\(s x^2 + t x + u = b\)</span>. So our system of equations in this case looks like:</p>

<p><span class="math">\[s x_1^2 + t x_1 + u = b_1\]</span></p>

<p><span class="math">\[\cdots\]</span></p>

<p><span class="math">\[s x_n^2 + t x_n + u = b_2\]</span></p>

<p>Although these are quadratic equations, they are linear in the coefficients - because we&#8217;re actually looking for <span class="math">\(s\)</span>, <span class="math">\(t\)</span>, and <span class="math">\(u\)</span>, and that&#8217;s sufficient. Thus our matrix setup looks like</p>

<p><span class="math">\[\begin{pmatrix} x_0^2 & x_0 & 1 \\ x_1^2 & x_1 & 1 \\ & \cdots & \\ x_n^2 & x_n & 1 \end{pmatrix} \begin{pmatrix} s  \\ t \\ u \end{pmatrix} = \begin{pmatrix} b_1 \\ b_2 \\ \cdots \\ b_n \end{pmatrix}\]</span></p>

<p>We&#8217;re given the <span class="math">\(\textbf{x}\)</span> values so <span class="math">\(A\)</span> is trivial to construct.</p>

<p>Working through this example gives values of approximately <span class="math">\(s = 1.01\)</span>, <span class="math">\(t = 3.004\)</span>, <span class="math">\(u = -2.116\)</span>. This is relatively high error but given the noisiness of the data a reasonable fit.</p>

<h3 id="changeofvariables">Change of variables</h3>

<table style="float:left;margin-right:50px;"><thead><tr><th>x</th><th>y</th></tr></thead>
<tbody>
<tr><td>0.5</td><td>5.049256664530498</td>
<tr><td>1</td><td>8.182255633188568</td>
<tr><td>1.5</td><td>13.563177467810174</td>
<tr><td>2</td><td>22.14093239888445</td>
<tr><td>2.5</td><td>36.64461491006822</td>
<tr><td>3</td><td>60.24551100013749</td>
<tr><td>3.5</td><td>99.49271499668747</td>
</tbody></table>

<p><img align="right" src="3_Table3Points.png"></p>

<p>As demonstrated in the last example, part of the unreasonable effectiveness of least squares is its applications to problems that are only linear in the coefficients, but not linear in the independent variables. We can often accomplish this with a change in variables.</p>

<p>Consider <span class="math">\(b = k e^x\)</span>. Before we try least squares approximation, we first have to transform it into a linear function. Taking the log of both sides:</p>

<p><span class="math" style="margin-left:50px"><span class="math">\(\log b = \log (k e^x)\)</span></span></p>

<p><span class="math" style="margin-left:50px"><span class="math">\(\log b = \log k + \log e^x\)</span></span></p>

<p><span class="math" style="margin-left:50px"><span class="math">\(\log b = \log k + x\)</span></span></p>

<p><span class="math" style="margin-left:50px"><span class="math">\(\log b = c + x\)</span></span></p>

<p>With <span class="math">\(c = \log k\)</span>, <span class="math">\(z = \log b\)</span>, our matrix thus stands:</p>

<p><span class="math">\(\begin{pmatrix} x_0 & 1 \\ x_1 & 1 \\ & \cdots & \\ x_n & 1 \end{pmatrix} \begin{pmatrix} x \\ c  \end{pmatrix} = \begin{pmatrix} z_1 \\ z_2 \\ \cdots \\ z_n \end{pmatrix}\)</span></p>

<p>Solving this system gives us <span class="math">\(c\)</span> which we can exponentiate for our sought constant <span class="math">\(k\)</span>. Keep in mind though that we&#8217;re minimizing the linear error, not the error in the original problem. The fit will not be equally good over the domain. There are more sophisticated (that is, complicated) techniques for non-linear function approximation.</p>

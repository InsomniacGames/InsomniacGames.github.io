<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    <style>

    tbody tr:nth-child(odd) {
        background-color: #ccc;
    }
    table {
        border: 1px solid black;
        border-collapse: collapse;
    }
    tr td {
        border-bottom:1pt solid black;
        padding:6px;
    }
    </style>
</head>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: { extensions: ["AMSsymbols.js"] },
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<h2 id="weightedlinearsquares">Weighted Linear Squares</h2>

<p>In our examples heretofore we&#8217;ve been assuming that each measurement - each row in <span class="math">\(\textbf{y}\)</span> - has equal accuracy. But sometimes we know that some measurements (and hence data points) are more reliable than others. To bias the final <span class="math">\(\textbf{x}^*\)</span> to work harder to minimize the error for those measurements, we can add a weight to equations that calculate the residual.</p>

<p>Consider the sum of squares error <span class="math">\(E^2 = \sum_i^n (y_i-a_i x_i)^2\)</span>. Weighting the individual datapoints consists of multiplying each equation with an additional paramters, <span class="math">\(w_i\)</span>, such that <span class="math">\(w_i > w_j\)</span> implies that we have more confidence in data point <span class="math">\(i\)</span> than <span class="math">\(j\)</span>. Ultimately this gives us <span class="math">\(E^2 = \sum_i^n w_i (y_i-a_i x_i)^2\)</span>, or in matrix form this is equivalent to <span class="math">\((\textbf{y}-Ax)^T W (\textbf{y}-Ax)\)</span> where</p>

<p><span class="math">\(W = \begin{pmatrix}
w_1 & 0 & \cdots \\
0 & w_2 & \cdots \\
& \ddots &  \\
\cdots & \cdots & w_3 \\
\end{pmatrix}\)</span>.</p>

<p>Performing least squares with a change of variables, <span class="math">\(\tilde{A} = AW\)</span>, <span class="math">\(\tilde{\textbf{y}} = W\textbf{y}\)</span> gives us the <em>weighted normal equations</em>:</p>

<p><span class="math">\((\tilde{A}^T\tilde{A})\textbf{x} =\tilde{A}^T\tilde{\textbf{b}}\)</span>.</p>

<p><span class="math">\((A^TW^TWA)\textbf{x} =A^TW^TW\textbf{b}\)</span>.</p>

<p>Alternatively, equating the derivative of the residual to 0, <span class="math">\(\frac{dE^2}{d\textbf{x}}=0\)</span>, will produce the same equations.</p>

<h2 id="equalityconstrainedlinearsquares">Equality constrained Linear Squares</h2>

<p>All the examples we&#8217;ve been looking at so far have been unconstrained: there are no restrictions on the values that <span class="math">\(\textbf{x}\)</span> can take. We can add equality constraints to our <span class="math">\(\min |A\textbf{x}=\textbf{y}|^2\)</span> easily, that is, with an analytic solution. (From <a href="http://livebooklabs.com/keeppies/c5a5868ce26b8125/6267fbb41c3f1521">Optimization models and applications</a>):</p>

<p>We express our equality constraints as the equation <span class="math">\(C\textbf{x} = \textbf{d}\)</span>. Assume that there is at least one feasible solution <span class="math">\(\textbf{x}_0\)</span> for <span class="math">\(C\textbf{x}=d\)</span>. Any attempt to move <span class="math">\(\textbf{x}\)</span> for the purpose of minimizing <span class="math">\(|A\textbf{x}=\textbf{y}|^2\)</span> by some vector <span class="math">\(\textbf{s}\)</span> must result in <span class="math">\(C(\textbf{x}_0 + \textbf{s}) = \textbf{d} \implies C\textbf{x}_0 + C\textbf{s} = \textbf{d}\)</span> to be correct. Since we must match <span class="math">\(C\textbf{x}_0 = \textbf{d}\)</span> exactly, <span class="math">\(C\textbf{s}\)</span> must be 0. Any attempt to change <span class="math">\(\textbf{x}\)</span> must occur along the nullspace of <span class="math">\(C\)</span>.</p>

<p>Let <span class="math">\(N\)</span> be the nullspace of <span class="math">\(C\)</span>. We can rephrase the above by saying we&#8217;re looking to minimize <span class="math">\(|A\textbf{x}=\textbf{y}|^2\)</span> where <span class="math">\(\textbf{x} = \textbf{x}_0 + N\textbf{z}\)</span>, <span class="math">\(\textbf{z}\)</span> is an arbitrary vector. Therefore the problem changes to minimize <span class="math">\(|\tilde{A}\textbf{z}=\tilde{\textbf{y}}|^2\)</span> where <span class="math">\(\tilde{A}=AN\)</span> and <span class="math">\(\tilde{y}=y-A\textbf{x}_0\)</span>. At this point we can plug our equation into our familiar normal equations and derive <span class="math">\(\textbf{x}\)</span>.</p>

<h3 id="anexample">An example</h3>

<p><img align="right" src="4_EllipsePoints.png"></p>

<p>Here&#8217;s an example adapted from <a href="http://holyshit.tech/statistical-learning/2016/11/06/least-squares.html">Flavio Truzzi&#8217;s article on Least Squares</a>. Take these set of data points which were generated by adding random noise about the ellipse <span class="math">\(\frac{1}{2}x^2 + \frac{1}{3}x^2 = 1\)</span> rotated by 20 degrees.</p>

<p>As a reminder there are multiple representations of the same ellipse. Truzzi uses the parametric form of a rotated ellipse. About the origin, the parameteric form is:</p>

<p><span class="math">\(x(t) = a\ cos(t) cos(\theta) - b\ sin(t) sin(\theta)\)</span></p>

<p><span class="math">\(y(t) = a\ cos(t) sin(\theta) + b\ sin(t) cos(\theta)\)</span></p>

<p>in this specific case, </p>

<p><span class="math">\(x(t) = 2\ cos(t) cos(20^째) - 3\ sin(t) sin(20^째)\)</span></p>

<p><span class="math">\(y(t) = 2\ cos(t) sin(20^째) + 3\ sin(t) cos(20^째)\)</span></p>

<p>In standard form the coefficients are <span class="math">\(\frac{1}{a}\)</span>, <span class="math">\(\frac{1}{b}\)</span> respectively.</p>

<p>Solving the general case provides values for <span class="math">\(a\ cos(\theta)\)</span> and <span class="math">\(b\ sin(\theta)\)</span>, which is less than ideal. More problematic, it requires solving two least squares problems, one for x and y separately. In order to limit the solution with equality constraints we need to solve them together. Thankfully we can avoid these problems using the quadratic form of the same ellipse.</p>

<p><span class="math">\(a'x^2 + b'xy + c'y^2 = 1\)</span></p>

<p><span class="math">\(a'\)</span>, <span class="math">\(b'\)</span>, and <span class="math">\(c'\)</span> are related to the <span class="math">\(a\)</span>, <span class="math">\(b\)</span> and the angle of rotation in the parametric representation by a set of formulas described in Charles F. Van Loan in <a href="http://www.cs.cornell.edu/cv/OtherPdf/Ellipse.pdf">Using the ellipse to fit and enclose data points</a>.</p>

<p>Given a set of <span class="math">\(x\)</span>, <span class="math">\(y\)</span> data points, we can reconstruct <span class="math">\(a'\)</span>, <span class="math">\(b'\)</span>, and <span class="math">\(c'\)</span> by the following <span class="math">\(A\)</span> and <span class="math">\(\textbf{b}\)</span> least squares setup:</p>

<p><img align="left" src="4_EllipsePoints2.png"></p>

<p><span class="math">\(\begin{pmatrix} x_1^2 & x_1y_1 & y_1^2 \\ x_2^2 & x_2y_2 & y_2^2 \\ \cdots \\ x_n^2 & x_ny_n & y_n^2 \\ \end{pmatrix} \begin{pmatrix}a' \\ b' \\ c'\end{pmatrix} = \begin{pmatrix}1 \\ 1 \\ \cdots \\ 1\end{pmatrix}\)</span></p>

<p>Converting back to parametric form gives us our original values for <span class="math">\(a\)</span> and <span class="math">\(b\)</span>. We also get the angle of rotation factored individually. Pretty nice!</p>

<p>Now onto equality constraints. Let&#8217;s say we wanted to restrict out solutions to those where <span class="math">\(a = b\)</span>. In other words we&#8217;re looking for the circle that best fits this group of points.</p>

<p>In matrix form, this constraint is expressed as <span class="math">\(\begin{pmatrix}1 & 0 & -1 \\ 0 & 1 & 0\end{pmatrix} \textbf{x} = \textbf{0}\)</span>. The middle unknown multiplies <span class="math">\(xy\)</span> and is responsible for the rotation, which we&#8217;ve set to unconditionally equal 0 with the second row. The first row states we want the coefficients for <span class="math">\(x^2\)</span> and <span class="math">\(y^2\)</span> to be equal. The nullspace of our constraint matrix, <span class="math">\(N\)</span>, is <span class="math">\(\begin{pmatrix}1 \\ 0 \\ 1\end{pmatrix}\)</span>.</p>

<p><img align="right" src="4_EllipsePoints3.png"></p>

<p>By inspection, a feasible solution for <span class="math">\(x_0\)</span> is <span class="math">\(\begin{pmatrix}1 \\ 0 \\ 1\end{pmatrix}\)</span>. Thus the optimal solution can be found via least squares with <span class="math">\(A N = \begin{pmatrix} x_1^2 + y_1^2 \\ \cdots \\ x_n^2 + y_n^2 \\ \end{pmatrix}\)</span>, <span class="math">\(\tilde{\textbf{b}} = \textbf{b} - A\textbf{x}_0 = \begin{pmatrix} b_1 - x_1^2 + y_1^2 \\ \cdots \\ b_n - x_n^2 + y_n^2\end{pmatrix}\)</span>.</p>

<p>Since the arrived at solution is not for <span class="math">\(\textbf{x}\)</span> directly but <span class="math">\(\textbf{z}\)</span>, our final solution <span class="math">\(\textbf{x}\)</span> will be <span class="math">\(\textbf{x}_0 + N\textbf{z}\)</span>.</p>
